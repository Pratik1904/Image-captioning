{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom tensorflow.compat.v1.keras.backend import set_session\nimport keras \nimport sys, time, os, warnings\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuring the GPU memory to be used for training purposes","metadata":{}},{"cell_type":"code","source":"config = tf.compat.v1.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.95\nconfig.gpu_options.visible_device_list = \"0\"\nset_session(tf.compat.v1.Session(config=config))\n\ndef set_seed(sd=8):\n    from numpy.random import seed\n    from tensorflow import set_random_seed\n    import random as rn\n    \n    seed(sd)\n    rn.seed(sd)\n    set_random_seed(sd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## importing the Image Dataset and its respective Captions","metadata":{}},{"cell_type":"code","source":"## The location of the Flickr8K_ images\ndir_Flickr_jpg = \"../input/flickr8k/Images\"\n## The location of the caption file\ndir_Flickr_text = \"../input/flickr8k-text/Flickr8k.token.txt\"\n\njpgs = os.listdir(dir_Flickr_jpg)\nprint(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding the captions for each Image","metadata":{}},{"cell_type":"code","source":"#Finding the captions for each image.\nfile = open(dir_Flickr_text,'r', encoding='utf8')\ntext = file.read()\nfile.close()\n\n\ndatatxt = []\nfor line in text.split('\\n'):\n    col = line.split('\\t')\n    if len(col) == 1:\n        continue\n    w = col[0].split(\"#\") # Splitting the caption dataset at the required position\n    datatxt.append(w + [col[1].lower()])\n\ndf_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n\n\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())\nprint(df_txt[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting few Images and their captions from the dataset","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\nfrom IPython.display import display\nfrom PIL import Image\n\nnpic = 5  # Displaying 5 images from the dataset\nnpix = 224\ntarget_size = (npix, npix, 3)\n\ncount = 1 \nfig = plt.figure(figsize=(10,20))\n\nfor jpgfnm in uni_filenames[-5:]:\n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n    image_load = load_img(filename, target_size=target_size)\n    \n    ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ax = fig.add_subplot(npic, 2 , count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0, len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0, i, caption, fontsize=16)\n    count += 1\nplt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning captions for further Analysis","metadata":{}},{"cell_type":"code","source":"# Defining a function to calculate the top 5 words in all the captions available for the images\ndef df_word(df_txt):\n    vocabulary = []\n    for txt in df_txt.caption.values:\n        vocabulary.extend(txt.split())\n    print('Vocabulary Size: %d' % len(set(vocabulary)))\n    ct = Counter(vocabulary)\n    dfword = pd.DataFrame({'word':list(ct.keys()), \"count\":list(ct.values())})\n    dfword = dfword.sort_values(\"count\", ascending=False)\n    dfword = dfword.reset_index()[[\"word\", \"count\"]]\n    \n    return (dfword)\n\ndfword = df_word(df_txt)\ndfword.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning the captions\n* The caption dataset contains punctuations, singular words and numerical values that need to be cleaned before it fed to the model.","metadata":{}},{"cell_type":"code","source":"import string\ntext_original = \"I ate 8 burgers and 4 pizzas. it's 9:44 am. can you play chess with me\"\n\nprint(\"Origial sentense: \",text_original)\nprint(\"\\nRemoving Punctuations...\")\n\n# creating a function that removes punctuation in the sentences\ndef remove_punctuation(text_original):\n    text_without_punct = text_original.translate(str.maketrans('','',string.punctuation))\n    return text_without_punct\n\ntext_without_punct = remove_punctuation(text_original)\nprint(text_without_punct)\n\nprint(\"\\nRemoving a single character...\")\n\n# creating a function that removes single character\ndef removing_single_char(text):\n    text_len_greater_than_one = \"\"\n    for word in text.split():\n        if len(word) > 1:\n            text_len_greater_than_one += \" \" + word\n    return text_len_greater_than_one\n\ntext_len_greater_than_one = removing_single_char(text_without_punct)\nprint(text_len_greater_than_one)\n\nprint(\"\\nRemoving numeric values...\")\n\n# creating a function that removes numerical values\ndef remove_numeric(text, printTF=False):\n    text_without_num = \"\"\n    for word in text.split():\n        isalpha = word.isalpha()\n        if printTF:\n            print(\"     {:10} : {:}\".format(word, isalpha))\n        if isalpha:\n            text_without_num += \" \"+ word\n    return text_without_num\n\ntext_without_num = remove_numeric(text_len_greater_than_one, printTF=True)\nprint(text_without_num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's applying functions to our data(Image captions)","metadata":{}},{"cell_type":"code","source":"def text_clean(text_original):\n    \n    text = remove_punctuation(text_original)\n    text = removing_single_char(text)\n    text = remove_numeric(text)\n    \n    return text\n\nfor i, caption in enumerate(df_txt.caption.values):\n    newcaption = text_clean(caption)\n    df_txt['caption'].iloc[i] = newcaption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting top 50 words that occur in the Cleaned Dataset","metadata":{}},{"cell_type":"code","source":"topn = 50\n\ndef plthist(dfsub, title=\"Plotting top 50 words that occur in the Cleaned Dataset\"):\n    plt.figure(figsize=(20,3))\n    plt.bar(dfsub.index, dfsub[\"count\"])\n    plt.yticks(fontsize=18)\n    plt.xticks(dfsub.index, dfsub[\"word\"], rotation=90, fontsize=18)\n    plt.title(title, fontsize=20)\n    plt.show()\n    \ndfword = df_word(df_txt)\nplthist(dfword.iloc[:topn,:], title=\"50 most frequently appearing words\")\nplthist(dfword.iloc[-topn:,:], title=\"50 least frequently appearing words\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding start and end sequence tokens for each captions\n* Start and End Sequence has to be added to the tokens so that it's easier to identify the captions for the image as each of them are of different length","metadata":{}},{"cell_type":"code","source":"from copy import copy\ndef add_tokens(captions):\n    caps = []\n    for txt in captions:\n        txt = \"startseq\" + txt + \" endseq\"\n        caps.append(txt)\n    return caps\ndf_txt0 = copy(df_txt)\ndf_txt0[\"caption\"] = add_tokens(df_txt[\"caption\"])\ndf_txt0.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading VGG16 model and weights to extract features from the images\n* The pre-trained weights for the VGG-16 model can be downloade from [here](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5)","metadata":{}},{"cell_type":"code","source":"from keras.applications import VGG16\n\nmodel_vgg = VGG16(include_top=True, weights=None)\nmodel_vgg.load_weights(\"../input/vgg16-pretrained-model/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\nmodel_vgg.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deleting the last layer of the Model\n* we only need feature extraction that's why we gonna exclude last layer of VGG-16 beacuse this layer is for object classification.","metadata":{}},{"cell_type":"code","source":"from keras import models\nmodel = Sequential()\nfor layer in model_vgg.layers[:-1]: # go through until last layer\n    model.add(layer)\nfinal_vgg_model = models.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n\nfinal_vgg_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction\n* the features are extracted from all the images in the dataset. VGG-16 model gives out 4096 features from the input image of size 224 * 224","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import load_img,  img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom collections import OrderedDict\n\nimages = OrderedDict()\nnpix = 224  #image size fixed at 224 because VGG16 model has been pre-trained to take that size.\ntarget_size = (npix, npix, 3)\ndata = np.zeros((len(jpgs), npix, npix, 3))\nfor i, name in enumerate(jpgs):\n    filename = dir_Flickr_jpg + '/' + name\n    image = load_img(filename, target_size=target_size)\n    image = img_to_array(image)\n    nimage = preprocess_input(image)\n    \n    y_pred = final_vgg_model.predict(nimage.reshape((1,)+ nimage.shape[:3]))\n    images[name] = y_pred.flatten()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## plotting similar images from the dataset\n* For this we have to first create a cluster and find which images are belongs together. hence PCA is used to reduce the dimensions of the features which we got from VGG-16 feature extraction from 4096 to 2.\n* first the cluster are plotted and few examples are taken from the bunch for displaying","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nencoder = np.array(list(images.values()))\n\npca = PCA(n_components=2)\n\ny_pca = pca.fit_transform(encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## some selected pictures that are cretaing clusters\n# these are just to display the related images from the dataset\n\npicked_picture = OrderedDict()\npicked_picture[\"yellow\"] = [1293, 3389, 6269, 5585, 2361]\npicked_picture[\"green\"] = [2336, 3585, 1790, 5377, 7504]\npicked_picture[\"magenta\"] = [2170, 4732, 761, 4851, 2820]\npicked_picture[\"blue\"] = [4559, 1850, 410, 401, 3825]\npicked_picture[\"purple\"] = [5074, 2563, 6545, 4978, 7895]\npicked_picture[\"red\"] = [6360, 5979, 7205, 5340, 5138]\n\nfig, ax = plt.subplots(figsize=(15,15))\nax.scatter(y_pca[:,0], y_pca[:,1], c=\"white\")\n\nfor irow in range(y_pca.shape[0]):\n    ax.annotate(irow, y_pca[irow,:], color=\"black\", alpha=0.5)\nfor color, irows in picked_picture.items():\n    for irow in irows:\n        ax.annotate(irow, y_pca[irow, :], color=color)\nax.set_xlabel(\"pca embedding 1\", fontsize=30)\nax.set_ylabel(\"pca embedding 2\", fontsize=30)\nplt.show()\n    \n## plot the images\nfig = plt.figure(figsize=(16,20))\ncount = 1\nfor color, irows in picked_picture.items():\n    for ivec in irows:\n        name = jpgs[ivec]\n        filename = dir_Flickr_jpg + '/' + name\n        image = load_img(filename, target_size=target_size)\n        ax = fig.add_subplot(len(picked_picture), 5, count, xticks=[], yticks=[])\n        count += 1\n        plt.imshow(image)\n        plt.title(\"{} ({})\".format(ivec, color))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Merging the images and the captions for training","metadata":{}},{"cell_type":"code","source":"dimages, keepindex = [], []\n# creating a dataframe where only first caption is taken for model training\ndf_txt0 = df_txt0.loc[df_txt0[\"index\"].values == '0',:]\n\nfor i, fnm in enumerate(df_txt0.filename):\n    if fnm in images.keys():\n        dimages.append(images[fnm])\n        keepindex.append(i)\n        \n# fname are the names of the image\nfnames = df_txt0[\"filename\"].iloc[keepindex].values\n# dcaptions contains captions of the images\ndcaptions = df_txt0[\"caption\"] .iloc[keepindex].values\n# dimages are the actual features of the images\ndimages = np.array(dimages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_txt0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the captions for further processing\n* as the model can not take texts as an input, they need to converted into vectors","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n## the maximum number of words in dictionary\nnb_words = 6000\ntokenizer = Tokenizer(nb_words=nb_words)\ntokenizer.fit_on_texts(dcaptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocubulary size : {}\".format(vocab_size))\ndtexts = tokenizer.texts_to_sequences(dcaptions)\nprint(dtexts[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the training and testing Data","metadata":{}},{"cell_type":"code","source":"prop_test, prop_val = 0.2, 0.2 \n\nN = len(dtexts)\nNtest, Nval = int(N*prop_test), int(N*prop_val)\n\ndef split_test_val_train(dtexts,Ntest,Nval):\n    return(dtexts[:Ntest], \n           dtexts[Ntest:Ntest+Nval],  \n           dtexts[Ntest+Nval:])\n\ndt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\ndi_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\nfnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding the Max Lenghth of caption","metadata":{}},{"cell_type":"code","source":"maxlen = np.max([len(text) for text in dtexts])\nminlen = np.min([len(text) for text in dtexts])\nprint(\"Min length of caption: {} and Max length of caption: {}\".format(minlen, maxlen))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing the captions and images as per the model standard input type","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef preprocessing(dtexts,dimages):\n    N = len(dtexts)\n    print(\"# captions/images = {}\".format(N))\n\n    assert(N==len(dimages)) # using assert to make sure that length of images and captions are always similar\n    Xtext, Ximage, ytext = [],[],[]\n    for text,image in zip(dtexts,dimages):\n        # zip() is used to create a tuple of iteratable items\n        for i in range(1,len(text)):\n            in_text, out_text = text[:i], text[i]\n            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()# using pad sequence to make the length of all captions equal\n            out_text = to_categorical(out_text,num_classes = vocab_size) # using to_categorical to \n\n            \n            Xtext.append(in_text)\n            Ximage.append(image)\n            ytext.append(out_text)\n\n    Xtext  = np.array(Xtext)\n    Ximage = np.array(Ximage)\n    ytext  = np.array(ytext)\n    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n    return(Xtext,Ximage,ytext)\n\n\nXtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\nXtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n# pre-processing is not necessary for testing data\n#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Buiding the LSTM model","metadata":{}},{"cell_type":"code","source":"from keras import layers\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nprint(vocab_size)\n\ndim_embedding = 64\n\ninput_image = layers.Input(shape=(Ximage_train.shape[1],))\nfimage = layers.Dense(256, activation='relu', name=\"ImageFeature\")(input_image)\n## sequence model\ninput_txt = layers.Input(shape=(maxlen,))\nftxt = layers.Embedding(vocab_size, dim_embedding, mask_zero=True)(input_txt)\nftxt = layers.LSTM(256, name=\"CaptionFeature\", return_sequences=True)(ftxt)\n\nse2 = Dropout(0.04)(ftxt)\nftxt = layers.LSTM(256, name=\"CaptionFeature2\")(se2)\n# combining model for decoder\n\ndecoder = layers.add([ftxt, fimage])\ndecoder = layers.Dense(256, activation=\"relu\")(decoder)\noutput = layers.Dense(vocab_size, activation='softmax')(decoder)\nmodel = models.Model(inputs=[input_image, input_txt], outputs=output)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the LSTM Model","metadata":{}},{"cell_type":"code","source":"# fit model\nfrom time import time\nfrom keras.callbacks import TensorBoard\n\ntensorboard = TensorBoard(log_dir=\"log/{}\".format(time()))\n\nhist = model.fit([Ximage_train, Xtext_train], ytext_train, epochs=6, verbose=2, batch_size=32, validation_data=([Ximage_val, Xtext_val], ytext_val), callbacks=[tensorboard])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label in [\"loss\",\"val_loss\"]:\n    plt.plot(hist.history[label], label=label)\nplt.legend()\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating captions on a small set of images\n* After the model finishes training we can test out its performance on the some of the test images to figure out if the generated captions are good enough. if the generated captions are good enough we can generate the captions for the whole dataset.","metadata":{}},{"cell_type":"code","source":"index_word = dict([(index, word) for word, index in tokenizer.word_index.items()])\n\ndef predict_caption(image):\n    \n    in_text = 'startseq'\n    \n    for iword in range(maxlen):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen)\n        yhat = model.predict([image, sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        newword = index_word[yhat]\n        in_text += \" \"+ newword\n        if newword == \"endseq\":\n            break\n    return in_text\n\nnpic = 5\nnpix = 224\ntarget_size = (npix, npix, 3)\ncount = 1\n\nfig = plt.figure(figsize=(10,20))\n\nfor jpgfnm, image_feature in zip(fnm_test[15:20], di_test[15:20]):\n    ## images\n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ## captions\n    caption = predict_caption(image_feature.reshape(1, len(image_feature)))\n    ax = fig.add_subplot(npic, 2, count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.text(0, 0.5, caption, fontsize=20)\n    count += 1\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the Model Performance\n* After the model is trained we have to test the models prediction capabilities on test dataset. Traditional accuracy metrics can't be used on predictions. For text evalutions we have a metric called as [BLEU Score](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).BLEU stands for Bilingual Evaluation Understidy. it is a score for comparing a candidate text to one or more reference text.\n* Example","metadata":{}},{"cell_type":"code","source":"hypothesis = \"I like dog\"\nhypothesis = hypothesis.split()\nreference = \"I do like dog\"\nreference = [reference.split()]\n\nfrom nltk.translate.bleu_score import sentence_bleu\nprint(\"BLEU={:4.3f}\".format(sentence_bleu(reference, hypothesis)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypothesis2 = \"I love dog!\".split()\nprint(\"BLEU={:4.3f}\".format(sentence_bleu(reference,  hypothesis2))) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating captions for the whole testset and finding BLEU score","metadata":{}},{"cell_type":"code","source":"index_word = dict([(index, word) for word, index in tokenizer.word_index.items()])\n\nnkeep = 5 \npred_strong, pred_weak, bleus = [], [], []\ncount = 0\n\nfor jpgfnm, image_feature, tokenized_text in zip(fnm_test, di_test, dt_test):\n    count += 1\n    if count % 200 == 0:\n        print(\"   {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n        \n    caption_true = [index_word[i] for i in tokenized_text]\n    caption_true = caption_true[1:-1]\n    ## captions\n    \n    caption = predict_caption(image_feature.reshape(1, len(image_feature)))\n    caption = caption.split()\n    caption = caption[1:-1]\n    \n    bleu = sentence_bleu([caption_true], caption)\n    bleus.append(bleu)\n    \n    if bleu > 0.7 and len(pred_strong) < nkeep:\n        pred_strong.append((bleu, jpgfnm, caption_true, caption))\n    elif bleu < 0.3 and len(pred_weak) < nkeep:\n        pred_weak.append((bleu, jpgfnm, caption_true, caption))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Good and Bad captions examples from the model\n* We can check out some of the images the generated caption's quality. some times due to the complex nature of the images the generated captions are not acceptable.","metadata":{}},{"cell_type":"code","source":"def plot_images(pred_weak):\n    def create_str(caption_true):\n        strue = \"\"\n        for s in caption_true:\n            strue += \" \" + s\n        return strue\n    npix = 224\n    target_size = (npix, npix, 3)\n    count = 1\n    fig = plt.figure(figsize=(10,20))\n    npic = len(pred_weak)\n    for pw in pred_weak:\n        bleu, jpgfnm, caption_true, caption = pw\n        ## image\n        filename = dir_Flickr_jpg + '/' + jpgfnm\n        image_load = load_img(filename, target_size=target_size)\n        ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n        ax.imshow(image_load)\n        count += 1\n        \n        caption_true = create_str(caption_true)\n        caption = create_str(caption)\n        \n        ax = fig.add_subplot(npic, 2, count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,1)\n        ax.text(0, 0.7, \"true:\" + caption_true, fontsize=20)\n        ax.text(0, 0.4, \"pred:\" + caption, fontsize=20)\n        ax.text(0, 0.1, \"BLEU: {}\".format(bleu), fontsize=20)\n        count += 1\n    plt.show()\n    \nprint(\"Weak Captions\")\nplot_images(pred_weak)\nprint(\"Strong Captions\")\nplot_images(pred_strong)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n* The model has been successfully trained to generate the captions as expected for the images. The caption generation has constantly been improved by fine tuning the model with different hyper parameter. Higher BLEU score indicates that the generated captions are very similar to those of the actual caption present on the images.\n* The validation loss falls upto 5th epoch and then increases afterwards, while the training loss still continues falling.\n\n\n* The following were the major outcomes and a observations of the training process and testing the model on the test data:\n\n* The validation loss increases after 5th epoch in most cases even though the training loss decreases over time. This indicates that the model is over fitting and the training needs to stop.\n* Higher BLEU score doesn't aways translate to better generated captions. If the model overfits on your training data, it will lead the model to go through details in the image and generate out captions which don't make sense. it can be seen in the strong and the weak captions generated above.","metadata":{}}]}